{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Baselines for VQA Counting\n",
        "\n",
        "### Options:\n",
        "\n",
        "#### a) Small:\n",
        "\n",
        "1. Blip-VQA-Base: https://huggingface.co/Salesforce/blip-vqa-base\n",
        "2. ViLt-finetuned-vqa: https://huggingface.co/dandelin/vilt-b32-finetuned-vqa\n",
        "4. OWL-ViT (Multimodal-Object Detection): https://huggingface.co/google/owlvit-base-patch32\n",
        "\n",
        "Rather not:\n",
        "5. Movie-ResNext: https://github.com/facebookresearch/mmf/tree/main/projects/movie_mcan\n",
        "\n",
        "=> available on Facebook Repo with huge model zoo, so probably more effort to work with\n",
        "6. RCN (TallyQA): https://github.com/manoja328/tallyqacode/tree/master\n",
        "\n",
        "=> also just on GitHub. Doubts: Repo not popular, some parts like dependencies in config.py with regard to data etc. not clear and model probably outdated\n",
        "\n",
        "#### b) Big:\n",
        "1. Pali-Gemma-3b: https://huggingface.co/google/paligemma-3b-pt-224\n",
        "2. InternLm-XComposer2-vl-7b: https://huggingface.co/internlm/internlm-xcomposer2-vl-7b/tree/main\n",
        "3. MiniCPM-Llama3-V-2_5 (8.54B): https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5\n",
        "\n",
        "=> More a general multimodal LLM... also available for 2Billion parameters"
      ],
      "metadata": {
        "id": "GnZojzDkTFKA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3_2GUn3TEnU",
        "outputId": "2d192e82-781a-4ca8-f9f3-1ff577bec6c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoProcessor, AutoModelForVisualQuestionAnswering\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "# %cd /content/drive/My Drive/MAI_project"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define the data class\n",
        "class HighCountVQADataset(Dataset):\n",
        "  def __init__(self, data_root, json_name, transform=transforms.ToTensor(), mode='RGB'):\n",
        "    with open(os.path.join(data_root, json_name), 'r') as file:\n",
        "      self.data_points = json.load(file)\n",
        "\n",
        "    self.mode = mode\n",
        "    self.data_root = data_root\n",
        "    self.transform = transform\n",
        "    self.source_map = {'generate': 'coco', 'imported_vqa': 'coco',\n",
        "                       'tdiuc_templates': 'coco', 'amt': 'visual_genome',\n",
        "                       'imported_genome': 'visual_genome'}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data_points)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    data_point = self.data_points[idx]\n",
        "    img_path = os.path.join(self.data_root, self.source_map[data_point['data_source']], data_point['image'])\n",
        "\n",
        "\n",
        "    img = self.transform(Image.open(img_path).convert(self.mode))\n",
        "\n",
        "    answer = data_point['answer']\n",
        "    question = data_point['question']\n",
        "\n",
        "    return {'question': question, 'image': img, 'answer': answer}"
      ],
      "metadata": {
        "id": "zsM3dip8c4DF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# config\n",
        "\n",
        "batch_size = 1\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else 'cpu'\n",
        "model_name = \"Salesforce/blip-vqa-base\"\n",
        "data_root = \"/content/drive/MyDrive/MAI_project\"\n",
        "json_file = \"HighCountVQA.json\""
      ],
      "metadata": {
        "id": "YFpk1zRsguha"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# declare all necessary components (here just for inference)\n",
        "test_set = HighCountVQADataset(data_root=data_root,\n",
        "                               json_name=json_file)\n",
        "test_loader = DataLoader(test_set,\n",
        "                         batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "# Blip specific loading\n",
        "\n",
        "# processor instead of tokenizer -> contains BERT tokenizer + BLIP image processor\n",
        "processor = AutoProcessor.from_pretrained(model_name)\n",
        "model = AutoModelForVisualQuestionAnswering.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "1A1HVC_1280t"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_iterations = int(len(test_set) / batch_size)\n",
        "\n",
        "# data structures for tracking accuracies\n",
        "max_obj_number = 15\n",
        "total_count = 0\n",
        "acc_arr = np.zeros((2, max_obj_number))\n",
        "\n",
        "with torch.no_grad():\n",
        "  for idx, data in enumerate(tqdm(test_loader, total=total_iterations)):\n",
        "    questions = data['question']\n",
        "    imgs = data['image']\n",
        "    answers = np.array(data['answer'])\n",
        "\n",
        "    inputs = processor(imgs, questions, return_tensors=\"pt\").to(device)\n",
        "    # \"generate\" is only for inference to get the full answer instead of just the next token\n",
        "    outputs = model.generate(**inputs)\n",
        "    predictions = processor.batch_decode(outputs, skip_special_tokens=True)\n",
        "    predictions = np.array(predictions, dtype=np.int32)\n",
        "\n",
        "    # update accuracy counts\n",
        "    true_pred = (predictions == answers)\n",
        "\n",
        "    total_count += true_pred.sum()\n",
        "    # add 1 to each ground truth occurence\n",
        "    np.add.at(acc_arr[0], answers, 1)\n",
        "    # add 1 to each correct count of those\n",
        "    np.add.at(acc_arr[1], answers[true_pred], 1)\n",
        "\n",
        "single_acc = (acc_arr[1] / acc_arr[0]) * 100\n",
        "\n",
        "print(f'Total Accuracy: {(total_count / len(test_set))*100}%')\n",
        "for i in range(max_obj_number):\n",
        "  print(f'Accuracy for {i+1} objects: {single_acc[i]}%')"
      ],
      "metadata": {
        "id": "cmVerXmh7BRh",
        "outputId": "94713aed-2044-4697-9594-d97adca0341a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/61538 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  0%|          | 4/61538 [00:14<63:25:48,  3.71s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "invalid literal for int() with base 10: 'one'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-59097c673f16>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# update accuracy counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'one'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problems\n",
        "- MANY different image sizes -> I got the first 500 images and EVERY image has a different size\n",
        "\n",
        "=> either use a common transforms function (which resolution is fitting then as many models have different input sizes?) or only use batchsize of 1 as this only throws an error when PyTorch DataLoader tries to stack the images\n",
        "- Known problem between colab and drive: IO errors when folders contain to many files (between 5k-15k?) -> possible solution is to split data again into many subfolders\n",
        "\n",
        "- model outputs sometimes not numbers but 'one' etc. -> how to treat these cases?\n",
        "\n",
        "# ToDo's\n",
        "- address problems\n",
        "- Split data into train, val, test (once data is curated and save on drive to always have the same set)\n",
        "\n",
        "# Notes:\n",
        "- Data Point deleted: {\"image\": \"VG_100K/2358553.jpg\", \"answer\": 1, \"data_source\": \"imported_genome\", \"question\": , \"image_id\": 5184, \"question_id\": 20031300}\n"
      ],
      "metadata": {
        "id": "EaryCVL03oWS"
      }
    }
  ]
}