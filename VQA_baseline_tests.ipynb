{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Baselines for VQA Counting\n",
        "\n",
        "### Options:\n",
        "\n",
        "#### a) Small:\n",
        "\n",
        "1. Blip-VQA-Base: https://huggingface.co/Salesforce/blip-vqa-base\n",
        "2. ViLt-finetuned-vqa: https://huggingface.co/dandelin/vilt-b32-finetuned-vqa\n",
        "4. OWL-ViT (Multimodal-Object Detection): https://huggingface.co/google/owlvit-base-patch32\n",
        "\n",
        "Rather not:\n",
        "5. Movie-ResNext: https://github.com/facebookresearch/mmf/tree/main/projects/movie_mcan\n",
        "\n",
        "=> available on Facebook Repo with huge model zoo, so probably more effort to work with\n",
        "6. RCN (TallyQA): https://github.com/manoja328/tallyqacode/tree/master\n",
        "\n",
        "=> also just on GitHub. Doubts: Repo not popular, some parts like dependencies in config.py with regard to data etc. not clear and model probably outdated\n",
        "\n",
        "#### b) Big:\n",
        "1. Pali-Gemma-3b: https://huggingface.co/google/paligemma-3b-pt-224\n",
        "2. InternLm-XComposer2-vl-7b: https://huggingface.co/internlm/internlm-xcomposer2-vl-7b/tree/main\n",
        "3. MiniCPM-Llama3-V-2_5 (8.54B): https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5\n",
        "\n",
        "=> More a general multimodal LLM... also available for 2Billion parameters"
      ],
      "metadata": {
        "id": "GnZojzDkTFKA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3_2GUn3TEnU",
        "outputId": "1d14e0c3-7b5b-4e1b-8520-909bffccb832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1LQRkg9gduygZjO84k_knGQGpy1qEPTp6/Colab Notebooks\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoProcessor, AutoModelForVisualQuestionAnswering\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "# %cd /content/drive/My Drive/MAI_project"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define the data class\n",
        "class HighCountVQADataset(Dataset):\n",
        "  def __init__(self, data_root, json_name, transform=lambda x: x, mode='RGB'):\n",
        "    with open(os.path.join(data_root, json_name), 'r') as file:\n",
        "      self.data_points = json.load(file)\n",
        "\n",
        "    self.mode = mode\n",
        "    self.data_root = data_root\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data_points)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    data_point = self.data_points[idx]\n",
        "    # ToDo: Adjust folder names of images to match the data_source field\n",
        "    img_path = os.path.join(self.data_root, data_point['data_source'], data_point['image'])\n",
        "    img = self.transform(Image.open(img_path).convert(self.mode))\n",
        "    answer = data_point['answer']\n",
        "    question = data_point['question']\n",
        "\n",
        "    return {'question': question, 'image': img, 'answer': answer}"
      ],
      "metadata": {
        "id": "zsM3dip8c4DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter\n",
        "batch_size=16\n",
        "device=\"cuda:0\"\n",
        "model_name = \"Salesforce/blip-vqa-base\"\n",
        "\n",
        "# declare all necessary components (here just for inference)\n",
        "test_set = HighCountVQADataset(data_root=\"/content/drive/My Drive/MAI_project\",\n",
        "                               json_name='HighCountVQA.json')\n",
        "test_loader = DataLoader(test_set,\n",
        "                         batchsize=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "# Blip specific loading\n",
        "\n",
        "# processor instead of tokenizer -> contains BERT tokenizer + BLIP image processor\n",
        "processor = AutoProcessor.from_pretrained(model_name)\n",
        "model = AutoModelForVisualQuestionAnswering.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "1A1HVC_1280t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_iterations = int(len(test_set) / batch_size)\n",
        "\n",
        "# data structures for tracking accuracies\n",
        "max_obj_number = 15\n",
        "total_count = 0\n",
        "distro_dict = {f'{i+1}':0 for i in range(max_obj_number)}\n",
        "count_dict = {f'{i+1}':0 for i in range(max_obj_number)}\n",
        "\n",
        "with torch.no_grad():\n",
        "  for idx, data in enumerate(tqdm(test_loader, total=total_iterations)):\n",
        "    questions = data['question']\n",
        "    imgs = data['image']\n",
        "    answers = data['answer']\n",
        "\n",
        "    inputs = processor(imgs, questions, return_tensors=\"pt\").to(device)\n",
        "    # \"generate\" is only for inference to get the full answer instead of just the next token\n",
        "    outputs = model.generate(**inputs)\n",
        "    predictions = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # update accuracy counts\n",
        "    true_pred = (predictions == answers)\n",
        "    total_count += true_pred.sum()\n",
        "    for is_match, answer in zip(true_pred, answers):\n",
        "      distro_dict[answer] += 1\n",
        "      if is_match:\n",
        "        count_dict[answer] += 1\n",
        "\n",
        "print(f'Total Accuracy: {(total_count / len(test_set))*100}%')\n",
        "for i in range(max_obj_number):\n",
        "  print(f'Accuracy for {i+1} objects: {(count_dict[i+1] / distro_dict[i+1])*100}%')"
      ],
      "metadata": {
        "id": "cmVerXmh7BRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ToDo's\n",
        "- Check data again because of errors like: {\"image\": \"VG_100K/2358553.jpg\", \"answer\": 1, \"data_source\": \"imported_genome\", \"question\": , \"image_id\": 5184, \"question_id\": 20031300} => no question, throws error in loading\n",
        "- Adjust folder names of images to match the data_source field\n",
        "- Split data into train, val, test (once data is curated and save on drive to always have the same set)"
      ],
      "metadata": {
        "id": "EaryCVL03oWS"
      }
    }
  ]
}